{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME_tjwmxFCef"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Defining Data Locations\n",
        "trainval_image_dir = os.path.join('/kaggle/input/coco-image-caption', 'train2014', 'train2014')\n",
        "trainval_captions_dir = os.path.join('/kaggle/input/coco-image-caption', 'annotations_trainval2014', 'annotations')\n",
        "test_image_dir = os.path.join('/kaggle/input/coco-image-caption', 'val2017', 'val2017')\n",
        "test_captions_dir = os.path.join('/kaggle/input/coco-image-caption', 'annotations_trainval2017', 'annotations')\n",
        "\n",
        "trainval_captions_filepath = os.path.join(trainval_captions_dir, 'captions_train2014.json')\n",
        "test_captions_filepath = os.path.join(test_captions_dir, 'captions_val2017.json')\n",
        "\n",
        "# Splitting Data into Train and Validation Set\n",
        "#We'll be using 20% of train_2014 data to be used as our Validation Set and rest as Training Set\n",
        "\n",
        "\n",
        "all_filepaths = np.array([os.path.join(trainval_image_dir, f) for f in os.listdir(trainval_image_dir)])\n",
        "rand_indices = np.arange(len(all_filepaths))\n",
        "np.random.shuffle(rand_indices)\n",
        "\n",
        "split = int(len(all_filepaths)*0.8)\n",
        "\n",
        "train_filepaths, valid_filepaths = all_filepaths[rand_indices[:split]], all_filepaths[rand_indices[split:]]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_filepaths)}\")\n",
        "print(f\"Valid dataset size: {len(valid_filepaths)}\")\n",
        "\n",
        "# Processing Data\n",
        "#Here we'll be making train, valid and test dataframes\n",
        "\n",
        "\n",
        "with open(trainval_captions_filepath, 'r') as f:\n",
        "    trainval_data = json.load(f)\n",
        "\n",
        "trainval_captions_df = pd.json_normalize(trainval_data, \"annotations\")\n",
        "trainval_captions_df[\"image_filepath\"] = trainval_captions_df[\"image_id\"].apply(\n",
        "    lambda x: os.path.join(trainval_image_dir, 'COCO_train2014_'+format(x, '012d')+'.jpg')\n",
        ")\n",
        "\n",
        "def preprocess_captions(image_captions_df):\n",
        "\n",
        "#Preprocessing the captions \"\"\"\n",
        "\n",
        "    image_captions_df[\"preprocessed_caption\"] = \"[START] \" + image_captions_df[\"caption\"].str.lower().str.replace('[^\\w\\s]','') + \" [END]\"\n",
        "    return image_captions_df\n",
        "\n",
        "train_captions_df = trainval_captions_df[trainval_captions_df[\"image_filepath\"].isin(train_filepaths)]\n",
        "train_captions_df = preprocess_captions(train_captions_df)\n",
        "valid_captions_df = trainval_captions_df[trainval_captions_df[\"image_filepath\"].isin(valid_filepaths)]\n",
        "valid_captions_df = preprocess_captions(valid_captions_df)\n",
        "\n",
        "with open(test_captions_filepath, 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "test_captions_df = pd.json_normalize(test_data, \"annotations\")\n",
        "test_captions_df[\"image_filepath\"] = test_captions_df[\"image_id\"].apply(\n",
        "    lambda x: os.path.join(test_image_dir, format(x, '012d')+'.jpg')\n",
        ")\n",
        "test_captions_df = preprocess_captions(test_captions_df)\n",
        "\n",
        "train_captions_df.head()\n",
        "\n",
        "# Understanding Data\n",
        "\n",
        "sample_data = valid_captions_df.groupby(\"image_filepath\")[\"caption\"].agg(list).iloc[:5]\n",
        "\n",
        "fig, axes = plt.subplots(5, 2, figsize=(8,18))\n",
        "\n",
        "for ax_row, index, sample in zip(axes, sample_data.index, sample_data):\n",
        "\n",
        "    ax_row[0].imshow(Image.open(index))\n",
        "    ax_row[0].axis(\"off\")\n",
        "    text_y = 0.9\n",
        "    for cap in sample:\n",
        "        ax_row[1].text(0, text_y, cap, fontsize=14)\n",
        "        text_y -= 0.2\n",
        "    ax_row[1].axis(\"off\")\n",
        "\n",
        "n_samples = 1000\n",
        "\n",
        "train_image_stats_df = valid_captions_df.loc[:n_samples, \"image_filepath\"].apply(lambda x: Image.open(x).size)\n",
        "train_image_stats_df = pd.DataFrame(train_image_stats_df.tolist(), index=train_image_stats_df.index)\n",
        "train_image_stats_df.describe()\n",
        "\n",
        "train_vocabulary = train_captions_df[\"preprocessed_caption\"].str.split(\" \").explode().value_counts()\n",
        "print(len(train_vocabulary[train_vocabulary>=25]))\n",
        "\n",
        "# Understanding the Bert Tokenizer\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# Initialize an empty BERT tokenizer\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    #reserved_tokens=[\"[UNK]\", \"[START]\", \"[END]\", \"[PAD]\"],\n",
        "    unk_token=\"[UNK]\",\n",
        "    #trainer_params=None,\n",
        "    #vocab_size=8000,\n",
        "    clean_text=False,\n",
        "    lowercase=False,\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(\n",
        "    train_captions_df[\"preprocessed_caption\"].tolist(),\n",
        "    vocab_size=4000,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        ")\n",
        "\n",
        "# Encoding a sentence\n",
        "example_captions = valid_captions_df[\"preprocessed_caption\"].iloc[:10].tolist()\n",
        "example_tokenized_captions = tokenizer.encode_batch(example_captions)\n",
        "\n",
        "for caption, tokenized_cap in zip(example_captions, example_tokenized_captions):\n",
        "    print(f\"{caption} -> {tokenized_cap.tokens}\")\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n",
        "\n",
        "for token in [\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\"]:\n",
        "    print(f\"{token} -> {vocab[token]}\")\n",
        "\n",
        "# Defining the `tf.data.Dataset` for image captioning\n",
        "\n",
        "def parse_image(filepath, resize_height, resize_width):\n",
        "    image = tf.io.read_file(filepath)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [resize_height, resize_width])\n",
        "    image = image*2.0 - 1.0\n",
        "    return image\n",
        "\n",
        "def generate_tokenizer(captions_df, n_vocab):\n",
        "    \"\"\" Generate the tokenizer with given captions \"\"\"\n",
        "\n",
        "    # Define the tokenizer\n",
        "    tokenizer = BertWordPieceTokenizer(\n",
        "        unk_token=\"[UNK]\",\n",
        "        clean_text=False,\n",
        "        lowercase=False,\n",
        "    )\n",
        "\n",
        "    # Train the tokenizer\n",
        "    tokenizer.train_from_iterator(\n",
        "        captions_df[\"preprocessed_caption\"].tolist(),\n",
        "        vocab_size=n_vocab,\n",
        "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "    )\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def generate_tf_dataset(image_captions_df, tokenizer=None, n_vocab=5000, pad_length=33, batch_size=32, training=False):\n",
        "   # \"\"\" Generate the tf.data.Dataset\"\"\"\n",
        "\n",
        "    # If the tokenizer is not available, create one\n",
        "    if not tokenizer:\n",
        "        tokenizer = generate_tokenizer(image_captions_df, n_vocab)\n",
        "\n",
        "    # Get the caption IDs using the tokenizer\n",
        "    image_captions_df[\"caption_token_ids\"] = [enc.ids for enc in tokenizer.encode_batch(image_captions_df[\"preprocessed_caption\"])]\n",
        "\n",
        "    vocab = tokenizer.get_vocab()\n",
        "\n",
        "    # Add the padding to short sentences and truncate long ones\n",
        "    image_captions_df[\"caption_token_ids\"] = image_captions_df[\"caption_token_ids\"].apply(\n",
        "        lambda x: x+[vocab[\"[PAD]\"]]*(pad_length - len(x) + 2) if pad_length + 2 >= len(x) else x[:pad_length + 1] + [x[-1]]\n",
        "    )\n",
        "\n",
        "    # Create a dataset with images and captions\n",
        "    dataset = tf.data.Dataset.from_tensor_slices({\n",
        "        \"image_filepath\": image_captions_df[\"image_filepath\"],\n",
        "        \"caption_token_ids\": np.array(image_captions_df[\"caption_token_ids\"].tolist())\n",
        "    })\n",
        "\n",
        "    # Each sample in our dataset consists of\n",
        "    # (image, caption token IDs, position IDs), (caption token IDs offset by 1)\n",
        "    dataset = dataset.map(\n",
        "        lambda x: (\n",
        "            (parse_image(x[\"image_filepath\"], 224, 224), x[\"caption_token_ids\"][:-1], tf.range(pad_length+1, dtype='float32')), x[\"caption_token_ids\"]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Shuffle and batch data in the training mode\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(buffer_size=batch_size*10)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    return dataset, tokenizer\n",
        "\n",
        "n_vocab=4000\n",
        "batch_size=2\n",
        "sample_dataset, sample_tokenizer = generate_tf_dataset(train_captions_df, n_vocab=n_vocab, pad_length=10, batch_size=batch_size, training=True)\n",
        "for i in sample_dataset.take(1):\n",
        "    print(i)\n",
        "\n",
        "# Defining the model\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "image_input = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "image_encoder = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\", trainable=False)\n",
        "image_features = image_encoder(image_input)\n",
        "print(f\"Final representation shape: {image_features.shape}\")\n",
        "\n",
        "## The Text Decoder Transformer\n",
        "##Here we define the text decoder. It takes the final image representation of ViT and concatenate that with caption IDs. Then we predict caption token ID from the next time step with the decoder.\n",
        "\n",
        "\n",
        "class SelfAttentionLayer(tf.keras.layers.Layer):\n",
        "#Defines the computations in the self attention layer\n",
        "\n",
        "    def __init__(self, d):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        # Feature dimensionality of the output\n",
        "        self.d = d\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Query weight matrix\n",
        "        self.Wq = self.add_weight(\n",
        "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
        "            trainable=True, dtype='float32'\n",
        "        )\n",
        "        # Key weight matrix\n",
        "        self.Wk = self.add_weight(\n",
        "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
        "            trainable=True, dtype='float32'\n",
        "        )\n",
        "        # Value weight matrix\n",
        "        self.Wv = self.add_weight(\n",
        "            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n",
        "            trainable=True, dtype='float32'\n",
        "        )\n",
        "\n",
        "    def call(self, q_x, k_x, v_x, mask=None):\n",
        "\n",
        "        q = tf.matmul(q_x,self.Wq) #[None, t, d]\n",
        "        k = tf.matmul(k_x,self.Wk) #[None, t, d]\n",
        "        v = tf.matmul(v_x,self.Wv) #[None, t, d]\n",
        "\n",
        "        # Computing the final output\n",
        "        h = tf.keras.layers.Attention(causal=True)([\n",
        "            q, #q\n",
        "            v, #v\n",
        "            k, #k\n",
        "        ], mask=[None, mask]) # [None, t, t] . [None, t, d] => [None, t, d]\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "   # \"\"\" The Decoder layer \"\"\"\n",
        "\n",
        "    def __init__(self, d, n_heads):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        # Feature dimensionality\n",
        "        self.d = d\n",
        "\n",
        "        # Dimensionality of a head\n",
        "        self.d_head = int(d/n_heads)\n",
        "\n",
        "        # Number of heads\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Actual attention heads\n",
        "        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(self.n_heads)]\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1_layer = tf.keras.layers.Dense(512, activation='relu')\n",
        "        self.fc2_layer = tf.keras.layers.Dense(d)\n",
        "\n",
        "        self.add_layer = tf.keras.layers.Add()\n",
        "        self.norm1_layer = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2_layer = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "\n",
        "    def _compute_multihead_output(self, x):\n",
        "      #  \"\"\" Computing the multi head attention output\"\"\"\n",
        "        outputs = [head(x, x, x) for head in self.attn_heads]\n",
        "        outputs = tf.concat(outputs, axis=-1)\n",
        "        return outputs\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "\n",
        "        # Multi head attention layer output\n",
        "        h1 = self._compute_multihead_output(x)\n",
        "\n",
        "        h1_add = self.add_layer([x, h1])\n",
        "        h1_norm = self.norm1_layer(h1_add)\n",
        "\n",
        "        # Fully connected outputs\n",
        "        h2_1 = self.fc1_layer(h1_norm)\n",
        "        h2_2 = self.fc2_layer(h2_1)\n",
        "\n",
        "        h2_add = self.add_layer([h1, h2_2])\n",
        "        h2_norm = self.norm2_layer(h2_add)\n",
        "\n",
        "\n",
        "        return h2_norm\n",
        "\n",
        "\n",
        "# Input layer\n",
        "caption_input = tf.keras.layers.Input(shape=(None,))\n",
        "position_input = tf.keras.layers.Input(shape=(None,))\n",
        "d_model = 384\n",
        "\n",
        "# Token embeddings\n",
        "input_embedding = tf.keras.layers.Embedding(len(tokenizer.get_vocab()), d_model, mask_zero=True)\n",
        "\n",
        "# Position embeddings\n",
        "position_embedding = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.where(\n",
        "        tf.math.mod(tf.repeat(tf.expand_dims(x, axis=-1), d_model, axis=-1), 2)==0,\n",
        "        tf.math.sin(\n",
        "            #tf.repeat(tf.expand_dims(x, axis=-1), d_model, axis=-1) /\n",
        "            tf.expand_dims(x, axis=-1) /\n",
        "            10000**(2*tf.reshape(tf.range(d_model, dtype='float32'),[1,1, -1])/d_model)\n",
        "        ),\n",
        "        tf.math.cos(\n",
        "            tf.expand_dims(x, axis=-1) /\n",
        "            10000**(2*tf.reshape(tf.range(d_model, dtype='float32'),[1,1, -1])/d_model)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "# Combined token position embeddings\n",
        "embed_out = input_embedding(caption_input) + position_embedding(position_input)\n",
        "# Concatenate image caption and token embeddings\n",
        "image_caption_embed_out = tf.keras.layers.Concatenate(axis=1)([tf.expand_dims(image_features,axis=1), embed_out])\n",
        "\n",
        "# Generate hidden representation with Transformer decoder layer\n",
        "out = image_caption_embed_out\n",
        "for l in range(4):\n",
        "    out  = TransformerDecoderLayer(d_model, 64)(out)\n",
        "\n",
        "# Final prediction layer\n",
        "final_out = tf.keras.layers.Dense(n_vocab, activation='softmax')(out)\n",
        "\n",
        "# Define the final model and compile\n",
        "full_model = tf.keras.models.Model(inputs=[image_input, caption_input, position_input], outputs=final_out)\n",
        "full_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "\n",
        "full_model.summary()\n",
        "\n",
        "\"\"\"## Defining the Blue Metric\"\"\"\n",
        "\n",
        "import collections\n",
        "import math\n",
        "\n",
        "\n",
        "def _get_ngrams(segment, max_order):\n",
        "    ngram_counts = collections.Counter()\n",
        "    for order in range(1, max_order + 1):\n",
        "        for i in range(0, len(segment) - order + 1):\n",
        "            ngram = tuple(segment[i:i+order])\n",
        "            ngram_counts[ngram] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "\n",
        "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
        "                 smooth=False):\n",
        "\n",
        "    matches_by_order = [0] * max_order\n",
        "    possible_matches_by_order = [0] * max_order\n",
        "    reference_length = 0\n",
        "    translation_length = 0\n",
        "    for (references, translation) in zip(reference_corpus,\n",
        "                                           translation_corpus):\n",
        "        reference_length += min(len(r) for r in references)\n",
        "        translation_length += len(translation)\n",
        "\n",
        "        merged_ref_ngram_counts = collections.Counter()\n",
        "        for reference in references:\n",
        "            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
        "        translation_ngram_counts = _get_ngrams(translation, max_order)\n",
        "        overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
        "        for ngram in overlap:\n",
        "            matches_by_order[len(ngram)-1] += overlap[ngram]\n",
        "        for order in range(1, max_order+1):\n",
        "            possible_matches = len(translation) - order + 1\n",
        "            if possible_matches > 0:\n",
        "                possible_matches_by_order[order-1] += possible_matches\n",
        "\n",
        "        precisions = [0] * max_order\n",
        "        for i in range(0, max_order):\n",
        "            if smooth:\n",
        "                   precisions[i] = ((matches_by_order[i] + 1.) /\n",
        "                           (possible_matches_by_order[i] + 1.))\n",
        "            else:\n",
        "                if possible_matches_by_order[i] > 0:\n",
        "                    precisions[i] = (float(matches_by_order[i]) /\n",
        "                             possible_matches_by_order[i])\n",
        "                else:\n",
        "                    precisions[i] = 0.0\n",
        "\n",
        "        if min(precisions) > 0:\n",
        "            p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
        "            geo_mean = math.exp(p_log_sum)\n",
        "        else:\n",
        "            geo_mean = 0\n",
        "\n",
        "        ratio = float(translation_length) / reference_length\n",
        "\n",
        "        if ratio > 1.0:\n",
        "            bp = 1.\n",
        "        else:\n",
        "            bp = math.exp(1 - 1. / ratio)\n",
        "\n",
        "        bleu = geo_mean * bp\n",
        "\n",
        "        return (bleu, precisions, bp, ratio, translation_length, reference_length)\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "\n",
        "class BLEUMetric(object):\n",
        "\n",
        "    def __init__(self, tokenizer, name='bleu_metric', **kwargs):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "      #self.vocab = vocabulary\n",
        "      #self.id_to_token_layer = StringLookup(vocabulary=self.vocab, num_oov_indices=0, oov_token='[UNKUNK]', invert=True)\n",
        "\n",
        "    def calculate_bleu_from_predictions(self, real, pred):\n",
        "\n",
        "        # Get the predicted token IDs\n",
        "        pred_argmax = tf.argmax(pred, axis=-1)\n",
        "\n",
        "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
        "        pred_tokens = np.array([[self.tokenizer.id_to_token(pp) for pp in p] for p in pred_argmax])\n",
        "        real_tokens = tf.constant([[self.tokenizer.id_to_token(rr) for rr in r] for r in real])\n",
        "\n",
        "        def clean_text(tokens):\n",
        "\n",
        "            # 3. Strip the string of any extra white spaces\n",
        "            translations_in_bytes = tf.strings.strip(\n",
        "                        # 2. Replace everything after the eos token with blank\n",
        "                        tf.strings.regex_replace(\n",
        "                            # 1. Join all the tokens to one string in each sequence\n",
        "                            tf.strings.join(\n",
        "                                tf.transpose(tokens), separator=' '\n",
        "                            ),\n",
        "                        \"\\[END\\].*\", \"\"),\n",
        "                   )\n",
        "\n",
        "            # Decode the byte stream to a string\n",
        "            translations = np.char.decode( #\n",
        "                translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n",
        "            )\n",
        "\n",
        "            # If the string is empty, add a [UNK] token\n",
        "            # Otherwise get a Division by zero error\n",
        "            translations = [sent if len(sent)>0 else \"[UNK]\" for sent in translations ]\n",
        "\n",
        "            # Split the sequences to individual tokens\n",
        "            translations = np.char.split(translations).tolist()\n",
        "\n",
        "            return translations\n",
        "\n",
        "        # Get the clean versions of the predictions and real seuqences\n",
        "        pred_tokens = clean_text(pred_tokens)\n",
        "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
        "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
        "\n",
        "        # The compute_bleu method accpets the translations and references in the following format\n",
        "        # tranlation - list of list of tokens\n",
        "        # references - list of list of list of tokens\n",
        "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
        "\n",
        "        return bleu\n",
        "\n",
        "\n",
        "batch_size=96\n",
        "\n",
        "train_fraction = 0.6\n",
        "valid_fraction = 0.2\n",
        "\n",
        "tokenizer = generate_tokenizer(\n",
        "    train_captions_df, n_vocab=n_vocab\n",
        ")\n",
        "\n",
        "bleu_metric = BLEUMetric(tokenizer=tokenizer)\n",
        "\n",
        "sampled_validation_captions_df = valid_captions_df.sample(frac=valid_fraction)\n",
        "\n",
        "for e in range(5):\n",
        "    print(f\"Epoch: {e+1}\")\n",
        "\n",
        "    train_dataset, _ = generate_tf_dataset(\n",
        "        train_captions_df.sample(frac=train_fraction), tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, training=True\n",
        "    )\n",
        "    valid_dataset, _ = generate_tf_dataset(\n",
        "        sampled_validation_captions_df, tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, training=False\n",
        "    )\n",
        "\n",
        "    full_model.fit(\n",
        "        train_dataset,\n",
        "        epochs=1\n",
        "    )\n",
        "\n",
        "    valid_loss, valid_accuracy, valid_bleu = [], [], []\n",
        "    for vi, v_batch in enumerate(valid_dataset):\n",
        "        print(f\"{vi+1} batches processed\", end='\\r')\n",
        "        loss, accuracy = full_model.test_on_batch(v_batch[0], v_batch[1])\n",
        "        batch_predicted = full_model(v_batch[0])\n",
        "        bleu_score = bleu_metric.calculate_bleu_from_predictions(v_batch[1], batch_predicted)\n",
        "        valid_loss.append(loss)\n",
        "        valid_accuracy.append(accuracy)\n",
        "        valid_bleu.append(bleu_score)\n",
        "\n",
        "    print(\n",
        "        f\"\\nvalid_loss: {np.mean(valid_loss)} - valid_accuracy: {np.mean(valid_accuracy)} - valid_bleu: {np.mean(valid_bleu)}\"\n",
        "    )\n",
        "\n",
        "bleu_metric = BLEUMetric(tokenizer=tokenizer)\n",
        "\n",
        "test_dataset, _ = generate_tf_dataset(\n",
        "    test_captions_df, tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, training=False\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy, test_bleu = [], [], []\n",
        "for ti, t_batch in enumerate(test_dataset):\n",
        "    print(f\"{ti+1} batches processed\", end='\\r')\n",
        "    loss, accuracy = full_model.test_on_batch(t_batch[0], t_batch[1])\n",
        "    batch_predicted = full_model.predict_on_batch(t_batch[0])\n",
        "    bleu_score = bleu_metric.calculate_bleu_from_predictions(t_batch[1], batch_predicted)\n",
        "    test_loss.append(loss)\n",
        "    test_accuracy.append(accuracy)\n",
        "    test_bleu.append(bleu_score)\n",
        "\n",
        "print(\n",
        "    f\"\\ntest_loss: {np.mean(test_loss)} - test_accuracy: {np.mean(test_accuracy)} - test_bleu: {np.mean(test_bleu)}\"\n",
        ")\n",
        "\n",
        "n_samples = 10\n",
        "test_dataset, _ = generate_tf_dataset(\n",
        "    valid_captions_df.sample(n=n_samples), tokenizer=tokenizer, n_vocab=n_vocab, batch_size=n_samples, training=False\n",
        ")\n",
        "\n",
        "def generate_caption(model, image_input, tokenizer, n_samples):\n",
        "    # 2 -> [START]\n",
        "    batch_tokens = np.repeat(np.array([[2]]), n_samples, axis=0)\n",
        "\n",
        "    for i in range(30):\n",
        "        if np.all(batch_tokens[:,-1] == 3):\n",
        "            break\n",
        "\n",
        "        position_input = tf.repeat(tf.reshape(tf.range(i+1),[1,-1]), n_samples, axis=0)\n",
        "        probs = full_model((image_input, batch_tokens, position_input)).numpy()\n",
        "        batch_tokens = np.argmax(probs, axis=-1)\n",
        "\n",
        "    predicted_text = []\n",
        "    for sample_tokens in batch_tokens:\n",
        "        sample_predicted_token_ids = sample_tokens.ravel()\n",
        "        sample_predicted_tokens = []\n",
        "        for wid in sample_predicted_token_ids:\n",
        "            sample_predicted_tokens.append(tokenizer.id_to_token(wid))\n",
        "            if wid == 3:\n",
        "                break\n",
        "        sample_predicted_text = \" \".join([tok for tok in sample_predicted_tokens])\n",
        "        sample_predicted_text = sample_predicted_text.replace(\" ##\", \"\")\n",
        "        predicted_text.append(sample_predicted_text)\n",
        "\n",
        "    return predicted_text\n",
        "\n",
        "\n",
        "for batch in test_dataset.take(1):\n",
        "    (batch_image_input, _, _), batch_true_caption = batch\n",
        "\n",
        "batch_predicted_text = generate_caption(full_model, batch_image_input, tokenizer, n_samples)\n",
        "\n",
        "fig, axes = plt.subplots(n_samples, 2, figsize=(8,30))\n",
        "\n",
        "for i,(sample_image_input, sample_true_caption, sample_predicated_caption) in enumerate(zip(batch_image_input, batch_true_caption, batch_predicted_text)):\n",
        "\n",
        "    sample_true_caption_tokens  = [tokenizer.id_to_token(wid) for wid in sample_true_caption.numpy().ravel()]\n",
        "\n",
        "    sample_true_text = []\n",
        "    for tok in sample_true_caption_tokens:\n",
        "        sample_true_text.append(tok)\n",
        "        if tok == '[END]':\n",
        "            break\n",
        "\n",
        "    sample_true_text = \" \".join(sample_true_text).replace(\" ##\", \"\")\n",
        "    axes[i][0].imshow(((sample_image_input.numpy()+1.0)/2.0))\n",
        "    axes[i][0].axis('off')\n",
        "\n",
        "    true_annotation = f\"TRUE: {sample_true_text}\"\n",
        "    predicted_annotation = f\"PRED: {sample_predicated_caption}\"\n",
        "    axes[i][1].text(0, 0.75, true_annotation, fontsize=18)\n",
        "    axes[i][1].text(0, 0.25, predicted_annotation, fontsize=18)\n",
        "    axes[i][1].axis('off')\n",
        "\n",
        "# full_model.save(\"image_captioning_modelV2.h5\")\n",
        "\n",
        "# full_model.save_weights(\"image_captioning_weights.h5\")\n",
        "\n",
        "# full_model.load_weights(\"/kaggle/working/image_captioning_weights.h5\")\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import numpy as np\n",
        "import keras\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#assigning image paths to variables\n",
        "mask_data = \"../input/facemask-dataset/Mask/Mask/\"\n",
        "no_mask_data = \"../input/facemask-dataset/No Mask/No Mask/\"\n",
        "\n",
        "total_mask_images = os.listdir(mask_data)\n",
        "print(\"no of mask images:: {}\".format(len(total_mask_images)))\n",
        "total_nonmask_images = os.listdir(no_mask_data)\n",
        "print(\"no of non-mask images:: {}\".format(len(total_nonmask_images)))\n",
        "\n",
        "os.makedirs('./train/mask')\n",
        "os.makedirs('./train/no mask')\n",
        "os.makedirs('./test/mask')\n",
        "os.makedirs('./test/no mask')\n",
        "\n",
        "for images in random.sample(total_mask_images,100):\n",
        "    shutil.copy(mask_data+images, './train/mask')\n",
        "for images in random.sample(total_mask_images,30):\n",
        "    shutil.copy(mask_data+images, './test/mask')\n",
        "for images in random.sample(total_nonmask_images,100):\n",
        "    shutil.copy(no_mask_data+images, './train/no mask')\n",
        "for images in random.sample(total_nonmask_images,30):\n",
        "    shutil.copy(no_mask_data+images, './test/no mask')\n",
        "\n",
        "train_batch = ImageDataGenerator(rescale=1./255, zoom_range=0.2, horizontal_flip=True, vertical_flip=True, shear_range=0.2).\\\n",
        "            flow_from_directory('./train', target_size=(224,224), batch_size=32, class_mode = 'categorical')\n",
        "test_batch = ImageDataGenerator(rescale=1./255).\\\n",
        "            flow_from_directory('./test', target_size = (224,224), batch_size=32, class_mode='categorical')\n",
        "\n",
        "train_batch.class_indices\n",
        "\n",
        "class_mask = ['mask', 'no mask']\n",
        "\n",
        "#import vgg16\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "IMAZE_SIZE = [224,224]\n",
        "vgg = VGG16(input_shape=IMAZE_SIZE+[3], weights='imagenet', include_top=False)\n",
        "\n",
        "for layers in vgg.layers:\n",
        "    layers.trainable = False\n",
        "\n",
        "flatten_layer = keras.layers.Flatten()(vgg.output)\n",
        "prediction_layer = keras.layers.Dense(2, activation='softmax')(flatten_layer)\n",
        "\n",
        "model = keras.models.Model(inputs = vgg.input, outputs = prediction_layer)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "r = model.fit_generator(train_batch, validation_data=test_batch, epochs=5, steps_per_epoch=len(train_batch), validation_steps=len(test_batch))\n",
        "\n",
        "plt.plot(r.history['loss'], label = 'train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.plot(r.history['accuracy'], label = 'train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "\n",
        "model.save(f\"best_testfacemask.hdf5\")\n",
        "\n",
        "import keras.utils as imagel\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "img = imagel.load_img('../input/facemask-dataset/No Mask/No Mask/No Mask109.jpg', target_size=(224,224))\n",
        "x=imagel.img_to_array(img)\n",
        "x = np.expand_dims(x,0)\n",
        "y = preprocess_input(x)\n",
        "pred = class_mask[np.argmax(model.predict(y))]\n",
        "print(pred)\n",
        "plt.imshow(img)\n",
        "\n",
        "img = imagel.load_img('../input/facemask-dataset/Mask/Mask/Mask214.jpeg', target_size=(224,224))\n",
        "x=imagel.img_to_array(img)\n",
        "x = np.expand_dims(x,0)\n",
        "y = preprocess_input(x)\n",
        "pred = class_mask[np.argmax(model.predict(y))]\n",
        "print(pred)\n",
        "plt.imshow(img)\n",
        "\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import load_model\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Preprocess the independent image\n",
        "# def preprocess_image(image_path, image_size):\n",
        "#     img = tf.keras.preprocessing.image.load_img(image_path, target_size=(image_size, image_size))\n",
        "#     img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "#     img_array = tf.expand_dims(img_array, 0)\n",
        "#     return img_array / 255.0  # Rescale pixel values to [0, 1]\n",
        "\n",
        "# # # Step 2: Load the trained model\n",
        "# # model_path = '/content/drive/MyDrive/Datasets/best_test.hdf5'\n",
        "# # model = load_model(model_path)\n",
        "\n",
        "# # Step 3: Make predictions on the independent image\n",
        "# def predict(model, image_array, class_names):\n",
        "#     predictions = model.predict(image_array)\n",
        "#     predict_class = class_names[np.argmax(predictions[0])]\n",
        "#     confidence = round(100 * np.max(predictions[0]), 2)\n",
        "#     return predict_class, confidence\n",
        "\n",
        "# # Step 4: Provide the path to the independent image and test it\n",
        "# # independent_image_path = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg'  # Replace with the actual path to the image\n",
        "# IMAGE_SIZE = 256\n",
        "# # classes_names = ['class_1', 'class_2', 'class_3']  # Replace with your class names used during training\n",
        "\n",
        "# # preprocessed_image = preprocess_image(independent_image_path, IMAGE_SIZE)\n",
        "# # predicted_class, confidence = predict(model2, preprocessed_image, classes_names)\n",
        "\n",
        "model2=tf.keras.models.load_model('/kaggle/input/facemaskdataset/best_test.hdf5')\n",
        "\n",
        "def detectmask(imgpth):\n",
        "    img = imagel.load_img(imgpth, target_size=(224,224))\n",
        "    x=imagel.img_to_array(img)\n",
        "    x = np.expand_dims(x,0)\n",
        "    y = preprocess_input(x)\n",
        "    pred = class_mask[np.argmax(model.predict(y))]\n",
        "    return pred\n",
        "\n",
        "ans=detectmask('/kaggle/input/facemask/train/without_mask/0690.jpg')\n",
        "ans\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, (224, 224))  # Adjust the size as needed\n",
        "    image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "def generate_image_caption(image_path, model, tokenizer):\n",
        "    mask= detectmask(image_path)\n",
        "    image = load_and_preprocess_image(image_path)\n",
        "\n",
        "    image_input = np.expand_dims(image, axis=0)\n",
        "\n",
        "    n_samples = 1\n",
        "\n",
        "    batch_tokens = np.repeat(np.array([[2]]), n_samples, axis=0)\n",
        "\n",
        "    for i in range(30):\n",
        "        if np.all(batch_tokens[:, -1] == 3):\n",
        "            break\n",
        "\n",
        "        position_input = tf.repeat(tf.reshape(tf.range(i + 1), [1, -1]), n_samples, axis=0)\n",
        "        probs = model((image_input, batch_tokens, position_input)).numpy()\n",
        "        batch_tokens = np.argmax(probs, axis=-1)\n",
        "\n",
        "    predicted_tokens = batch_tokens[0]\n",
        "    predicted_text = []\n",
        "\n",
        "    for wid in predicted_tokens:\n",
        "        predicted_text.append(tokenizer.id_to_token(wid))\n",
        "        if wid == 3:\n",
        "            break\n",
        "\n",
        "    predicted_text = \" \".join(predicted_text).replace(\" ##\", \"\")\n",
        "\n",
        "    return predicted_text, image,mask\n",
        "\n",
        "def visualize_image_with_caption(image_path, model, tokenizer):\n",
        "    predicted_caption, image,mask = generate_image_caption(image_path, model, tokenizer)\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    if mask=='mask':\n",
        "        variable1 = predicted_caption\n",
        "        variable2 = mask\n",
        "\n",
        "#         title = f\"{variable1} with {variable2}\"\n",
        "        title = f\"{predicted_caption.replace('[END]', '')} with mask [END]\"\n",
        "        plt.title(title)\n",
        "    else:\n",
        "        plt.title(predicted_caption)\n",
        "    plt.show()\n",
        "\n",
        "# Usage example\n",
        "\n",
        "image_path = \"/kaggle/input/facemask-dataset/Mask/Mask/Mask118.jpeg\"  # Replace with the actual image path\n",
        "visualize_image_with_caption(image_path, full_model, tokenizer)\n",
        "\n",
        "# Usage example\n",
        "image_path = \"/kaggle/input/facemask/train/without_mask/0699.jpg\"  # Replace with the actual image path\n",
        "visualize_image_with_caption(image_path, full_model, tokenizer)\n",
        "\n",
        "image_path = \"/kaggle/input/facemask-dataset/Mask/Mask/Mask126.jpg\"  # Replace with the actual image path\n",
        "visualize_image_with_caption(image_path, full_model, tokenizer)\n",
        "\n",
        "image_path = \"/kaggle/input/facemask/train/with_mask/0235.jpg\"  # Replace with the actual image path\n",
        "visualize_image_with_caption(image_path, full_model, tokenizer)\n",
        "\n",
        "image_path = \"/kaggle/input/facemask/train/with_mask/0256.jpg\"  # Replace with the actual image path\n",
        "visualize_image_with_caption(image_path, full_model, tokenizer)\n",
        "\n",
        "image_path = \"/kaggle/input/facemask/train/without_mask/0717.jpg\"  # Replace with the actual image path\n",
        "visualize_image_with_caption(image_path, full_model, tokenizer)\n"
      ]
    }
  ]
}